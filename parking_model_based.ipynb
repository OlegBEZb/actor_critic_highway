{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parking_model_based.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/actor_critic_highway/blob/main/parking_model_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-oVNY_KTw6R"
      },
      "source": [
        "## Our challenge: Automated Parking System\n",
        "\n",
        "We consider the **parking-v0** task of the [highway-env](https://github.com/eleurent/highway-env) environment. It is a **goal-conditioned continuous control** task where an agent **drives a car** by controlling the gaz pedal and steering angle and must **park in a given location** with the appropriate heading.\n",
        "\n",
        "This MDP has several properties wich justifies using model-based methods:\n",
        "* The policy/value is highly dependent on the goal which adds a significant level of complexity to a model-free learning process, whereas the dynamics are completely independent of the goal and hence can be simpler to learn.\n",
        "* In the context of an industrial application, we can reasonably expect for safety concerns that the planned trajectory is required to be known in advance, before execution.\n",
        "\n",
        "###  Warming up\n",
        "We start with a few useful installs and imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMSuJEOfviP",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d798d09-465e-4ee5-8510-3e05a1a7317b"
      },
      "source": [
        "# Install environment and visualization dependencies \n",
        "!pip install highway-env\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -y\n",
        "\n",
        "# Environment\n",
        "import gym\n",
        "import highway_env\n",
        "\n",
        "# Models and computation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "# torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "import base64\n",
        "\n",
        "# IO\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting highway-env\n",
            "  Downloading highway_env-1.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 40 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from highway-env) (1.19.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from highway-env) (0.17.3)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from highway-env) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from highway-env) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->highway-env) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->highway-env) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->highway-env) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->highway-env) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->highway-env) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->highway-env) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->highway-env) (2018.9)\n",
            "Installing collected packages: pygame, highway-env\n",
            "Successfully installed highway-env-1.4 pygame-2.0.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.2\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,365 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,802 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,801 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [922 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Fetched 11.9 MB in 7s (1,635 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,281 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 1,281 kB in 2s (565 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Bu_Pqop0E7"
      },
      "source": [
        "We also define a simple helper function for visualization of episodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7yH4ucyB-3"
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video(path):\n",
        "    html = []\n",
        "    for mp4 in Path(path).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFtBY6JSqPFa"
      },
      "source": [
        "### Let's try it!\n",
        "\n",
        "Make the environment, and run an episode with random actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKZt9Cb1rJ6n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "00ff3ff6-670b-4b7b-dd53-2e6836ff7b12"
      },
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video('./video')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"video/openaigym.video.0.76.video000000.mp4\" autoplay \n",
              "                      loop controls style=\"height: 400px;\">\n",
              "                      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewG5f_essAS0"
      },
      "source": [
        "The environment is a `GoalEnv`, which means the agent receives a dictionary containing both the current `observation` and the `desired_goal` that conditions its policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIC98mGhr7v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7e528b-e28c-43ef-d2b8-0d81f5c3ed75"
      },
      "source": [
        "print(\"Observation format:\", obs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation format: {'observation': array([-0.2240445 ,  0.57381157, -0.17423251,  0.29511789, -0.50839333,\n",
            "        0.86112498]), 'achieved_goal': array([-0.2240445 ,  0.57381157, -0.17423251,  0.29511789, -0.50839333,\n",
            "        0.86112498]), 'desired_goal': array([-2.600000e-01, -1.400000e-01,  0.000000e+00,  0.000000e+00,\n",
            "        6.123234e-17, -1.000000e+00])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjYvyigH1Rv4",
        "outputId": "f63cae9e-a901-45a6-ae04-b67a75743e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dict(achieved_goal:Box(-inf, inf, (6,), float64), desired_goal:Box(-inf, inf, (6,), float64), observation:Box(-inf, inf, (6,), float64))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IQN0M8Ryto3",
        "outputId": "9fdd78ce-5742-4048-ebdc-522b47fddb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-1.0, 1.0, (2,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voagCILztJ3J"
      },
      "source": [
        "There is also an `achieved_goal` that won't be useful here (it only serves when the state and goal spaces are different, as a projection from the observation to the goal space)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCXb9ca3n7qi"
      },
      "source": [
        "# Modeling\n",
        "Try with both buffers\n",
        "\n",
        "Check DDPG, update to TD3  \n",
        "TQC?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZOJwxKLlx9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0268dcaa-a4bd-480c-d0e2-9b553352dacd"
      },
      "source": [
        "!pip install stable-baselines3\n",
        "!pip install sb3-contrib"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-1.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 92 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 161 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (1.9.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->stable-baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->stable-baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3) (2018.9)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.2.0\n",
            "Collecting sb3-contrib\n",
            "  Downloading sb3_contrib-1.2.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: stable-baselines3>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sb3-contrib) (1.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (3.2.2)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3>=1.2.0->sb3-contrib) (1.9.0+cu111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3>=1.2.0->sb3-contrib) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3>=1.2.0->sb3-contrib) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3>=1.2.0->sb3-contrib) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->stable-baselines3>=1.2.0->sb3-contrib) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.2.0->sb3-contrib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.2.0->sb3-contrib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.2.0->sb3-contrib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3>=1.2.0->sb3-contrib) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->stable-baselines3>=1.2.0->sb3-contrib) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3>=1.2.0->sb3-contrib) (2018.9)\n",
            "Installing collected packages: sb3-contrib\n",
            "Successfully installed sb3-contrib-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fthS9HdtxOHs"
      },
      "source": [
        "## Buffers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WVM6_W-hb__"
      },
      "source": [
        "from abc import ABC\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gym import spaces\n",
        "\n",
        "from stable_baselines3.common.preprocessing import get_action_dim, get_obs_shape\n",
        "from stable_baselines3.common.type_aliases import (\n",
        "    DictReplayBufferSamples,\n",
        ")\n",
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "\n",
        "try:\n",
        "    # Check memory used by replay buffer when possible\n",
        "    import psutil\n",
        "except ImportError:\n",
        "    psutil = None\n",
        "\n",
        "\n",
        "class DictReplayBufferBase(ABC):\n",
        "    \"\"\"\n",
        "    Dict Replay buffer used in off-policy algorithms like SAC/TD3.\n",
        "    :param buffer_size: Max number of element in the buffer\n",
        "    :param observation_space: Observation space\n",
        "    :param action_space: Action space\n",
        "    :param device:\n",
        "    :param n_envs: Number of parallel environments\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant\n",
        "        Disabled for now (see https://github.com/DLR-RM/stable-baselines3/pull/243#discussion_r531535702)\n",
        "    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n",
        "        separately and treat the task as infinite horizon task.\n",
        "        https://github.com/DLR-RM/stable-baselines3/issues/284\n",
        "    \"\"\"\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"\n",
        "        :return: The current size of the buffer\n",
        "        \"\"\"\n",
        "        if self.full:\n",
        "            return self.buffer_size\n",
        "        return self.pos\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"\n",
        "        Reset the buffer.\n",
        "        \"\"\"\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "    # TODO: if not used explicitly, may be removed\n",
        "    def extend(self, *args, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Add a new batch of transitions to the buffer\n",
        "        \"\"\"\n",
        "        # Do a for loop along the batch axis\n",
        "        for data in zip(*args):\n",
        "            self.add(*data)\n",
        "\n",
        "    def add(\n",
        "            self,\n",
        "            obs: Dict[str, np.ndarray],\n",
        "            next_obs: Dict[str, np.ndarray],\n",
        "            action: np.ndarray,\n",
        "            reward: np.ndarray,\n",
        "            done: np.ndarray,\n",
        "            infos: List[Dict[str, Any]],\n",
        "    ) -> None:\n",
        "        # Copy to avoid modification by reference\n",
        "        for key in self.observations.keys():\n",
        "            self.observations[key][self.pos] = np.array(obs[key]).copy()\n",
        "\n",
        "        for key in self.next_observations.keys():\n",
        "            self.next_observations[key][self.pos] = np.array(next_obs[key]).copy()\n",
        "\n",
        "        self.actions[self.pos] = np.array(action).copy()\n",
        "        self.rewards[self.pos] = np.array(reward).copy()\n",
        "        self.dones[self.pos] = np.array(done).copy()\n",
        "\n",
        "        if self.handle_timeout_termination:\n",
        "            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n",
        "\n",
        "        self.pos += 1\n",
        "        if self.pos == self.buffer_size:\n",
        "            self.full = True\n",
        "            self.pos = 0\n",
        "\n",
        "    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> DictReplayBufferSamples:\n",
        "\n",
        "        # Normalize if needed and remove extra dimension (we are using only one env for now)\n",
        "        obs_ = self._normalize_obs({key: obs[batch_inds, 0, :] for key, obs in self.observations.items()})\n",
        "        next_obs_ = self._normalize_obs({key: obs[batch_inds, 0, :] for key, obs in self.next_observations.items()})\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        observations = {key: self.to_torch(obs) for key, obs in obs_.items()}\n",
        "        next_observations = {key: self.to_torch(obs) for key, obs in next_obs_.items()}\n",
        "\n",
        "        return DictReplayBufferSamples(\n",
        "            observations=observations,\n",
        "            actions=self.to_torch(self.actions[batch_inds]),\n",
        "            next_observations=next_observations,\n",
        "            # Only use dones that are not due to timeouts\n",
        "            # deactivated by default (timeouts is initialized as an array of False)\n",
        "            dones=self.to_torch(self.dones[batch_inds] * (1 - self.timeouts[batch_inds])),\n",
        "            rewards=self.to_torch(self._normalize_reward(self.rewards[batch_inds], env)),\n",
        "        )\n",
        "\n",
        "    def to_torch(self, array: np.ndarray, copy: bool = True) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        Convert a numpy array to a PyTorch tensor.\n",
        "        Note: it copies the data by default\n",
        "        :param array:\n",
        "        :param copy: Whether to copy or not the data\n",
        "            (may be useful to avoid changing things be reference)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if copy:\n",
        "            return th.tensor(array).to(self.device)\n",
        "        return th.as_tensor(array).to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_obs(\n",
        "            obs: Union[np.ndarray, Dict[str, np.ndarray]],\n",
        "            env: Optional[VecNormalize] = None,\n",
        "    ) -> Union[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        if env is not None:\n",
        "            return env.normalize_obs(obs)\n",
        "        return obs\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_reward(reward: np.ndarray, env: Optional[VecNormalize] = None) -> np.ndarray:\n",
        "        if env is not None:\n",
        "            return env.normalize_reward(reward).astype(np.float32)\n",
        "        return reward\n",
        "\n",
        "\n",
        "class DictReplayBuffer(DictReplayBufferBase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            buffer_size: int,\n",
        "            observation_space: spaces.Space,\n",
        "            action_space: spaces.Space,\n",
        "            device: Union[th.device, str] = \"cpu\",\n",
        "            n_envs: int = 1,\n",
        "            handle_timeout_termination: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.buffer_size = buffer_size\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.obs_shape = get_obs_shape(observation_space)\n",
        "\n",
        "        self.action_dim = get_action_dim(action_space)\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "        self.device = device\n",
        "        self.n_envs = n_envs\n",
        "\n",
        "        assert isinstance(self.obs_shape, dict), \"DictReplayBuffer must be used with Dict obs space only\"\n",
        "        assert n_envs == 1, \"Replay buffer only support single environment for now\"\n",
        "\n",
        "        # Check that the replay buffer can fit into the memory\n",
        "        if psutil is not None:\n",
        "            mem_available = psutil.virtual_memory().available\n",
        "\n",
        "        self.observations = {\n",
        "            key: np.zeros((self.buffer_size, self.n_envs) + _obs_shape, dtype=observation_space[key].dtype)\n",
        "            for key, _obs_shape in self.obs_shape.items()\n",
        "        }\n",
        "        self.next_observations = {\n",
        "            key: np.zeros((self.buffer_size, self.n_envs) + _obs_shape, dtype=observation_space[key].dtype)\n",
        "            for key, _obs_shape in self.obs_shape.items()\n",
        "        }\n",
        "\n",
        "        # only 1 env is supported\n",
        "        self.actions = np.zeros((self.buffer_size, self.action_dim), dtype=action_space.dtype)\n",
        "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
        "        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
        "\n",
        "        # Handle timeouts termination properly if needed\n",
        "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
        "        self.handle_timeout_termination = handle_timeout_termination\n",
        "        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n",
        "\n",
        "        if psutil is not None:\n",
        "            obs_nbytes = 0\n",
        "            for _, obs in self.observations.items():\n",
        "                obs_nbytes += obs.nbytes\n",
        "\n",
        "            total_memory_usage = obs_nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
        "            if self.next_observations is not None:\n",
        "                next_obs_nbytes = 0\n",
        "                for _, obs in self.observations.items():\n",
        "                    next_obs_nbytes += obs.nbytes\n",
        "                total_memory_usage += next_obs_nbytes\n",
        "\n",
        "            if total_memory_usage > mem_available:\n",
        "                # Convert to GB\n",
        "                total_memory_usage /= 1e9\n",
        "                mem_available /= 1e9\n",
        "                warnings.warn(\n",
        "                    \"This system does not have apparently enough memory to store the complete \"\n",
        "                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n",
        "                )\n",
        "\n",
        "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None) -> DictReplayBufferSamples:\n",
        "        \"\"\"\n",
        "        Sample elements from the replay buffer.\n",
        "        :param batch_size: Number of element to sample\n",
        "        :param env: associated gym VecEnv\n",
        "            to normalize the observations/rewards when sampling\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Call self._get_samples with randomly selected indices\n",
        "        ### YOUR CODE HERE ###\n",
        "        upper_bound = self.buffer_size if self.full else self.pos\n",
        "        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n",
        "        return self._get_samples(batch_inds, env=env)\n",
        "        ######################\n",
        "\n",
        "\n",
        "# For convenience\n",
        "# that way, we can use string to select a strategy\n",
        "GOAL_STRATEGY_MAPPING = {\n",
        "    \"future\": 0,\n",
        "    \"final\": 1,\n",
        "    \"episode\": 2,\n",
        "}\n",
        "\n",
        "import warnings\n",
        "from collections import deque\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecEnv, VecNormalize\n",
        "\n",
        "\n",
        "def get_time_limit(env: VecEnv, current_max_episode_length: Optional[int]) -> int:\n",
        "    \"\"\"\n",
        "    Get time limit from environment.\n",
        "    :param env: Environment from which we want to get the time limit.\n",
        "    :param current_max_episode_length: Current value for max_episode_length.\n",
        "    :return: max episode length\n",
        "    \"\"\"\n",
        "    # try to get the attribute from environment\n",
        "    if current_max_episode_length is None:\n",
        "        try:\n",
        "            current_max_episode_length = env.get_attr(\"spec\")[0].max_episode_steps\n",
        "            # Raise the error because the attribute is present but is None\n",
        "            if current_max_episode_length is None:\n",
        "                raise AttributeError\n",
        "        # if not available check if a valid value was passed as an argument\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"The max episode length could not be inferred.\\n\"\n",
        "                \"You must specify a `max_episode_steps` when registering the environment,\\n\"\n",
        "                \"use a `gym.wrappers.TimeLimit` wrapper \"\n",
        "                \"or pass `max_episode_length` to the model constructor\"\n",
        "            )\n",
        "    return current_max_episode_length\n",
        "\n",
        "\n",
        "class HerReplayBufferBase(DictReplayBuffer):\n",
        "    \"\"\"\n",
        "    Hindsight Experience Replay (HER) buffer.\n",
        "    Paper: https://arxiv.org/abs/1707.01495\n",
        "    .. warning::\n",
        "      For performance reasons, the maximum number of steps per episodes must be specified.\n",
        "      In most cases, it will be inferred if you specify ``max_episode_steps`` when registering the environment\n",
        "      or if you use a ``gym.wrappers.TimeLimit`` (and ``env.spec`` is not None).\n",
        "      Otherwise, you can directly pass ``max_episode_length`` to the replay buffer constructor.\n",
        "    Replay buffer for sampling HER (Hindsight Experience Replay) transitions.\n",
        "    In the online sampling case, these new transitions will not be saved in the replay buffer\n",
        "    and will only be created at sampling time.\n",
        "    :param env: The training environment\n",
        "    :param buffer_size: The size of the buffer measured in transitions.\n",
        "    :param max_episode_length: The maximum length of an episode. If not specified,\n",
        "        it will be automatically inferred if the environment uses a ``gym.wrappers.TimeLimit`` wrapper.\n",
        "    :param goal_selection_strategy: Strategy for sampling goals for replay.\n",
        "        One of ['episode', 'final', 'future']\n",
        "    :param device: PyTorch device\n",
        "    :param n_sampled_goal: Number of virtual transitions to create per real transition,\n",
        "        by sampling new goals.\n",
        "    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n",
        "        separately and treat the task as infinite horizon task.\n",
        "        https://github.com/DLR-RM/stable-baselines3/issues/284\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            env: VecEnv,\n",
        "            buffer_size: int,\n",
        "            device: Union[th.device, str] = \"cpu\",\n",
        "            replay_buffer: Optional[DictReplayBufferBase] = None,\n",
        "            max_episode_length: Optional[int] = None,\n",
        "            n_sampled_goal: int = 4,\n",
        "            goal_selection_strategy: str = \"future\",\n",
        "            online_sampling: bool = True,\n",
        "            handle_timeout_termination: bool = True,\n",
        "    ):\n",
        "\n",
        "        super().__init__(buffer_size, env.observation_space, env.action_space, device,\n",
        "                         env.num_envs)\n",
        "\n",
        "        self.goal_selection_strategy = GOAL_STRATEGY_MAPPING[goal_selection_strategy.lower()]\n",
        "\n",
        "        # check if goal_selection_strategy is valid\n",
        "        assert self.goal_selection_strategy in list(\n",
        "            GOAL_STRATEGY_MAPPING.values()), \"Invalid goal selection strategy, please use one of 0,1 or 2\"\n",
        "\n",
        "        self.n_sampled_goal = n_sampled_goal\n",
        "        # if we sample her transitions online use custom replay buffer\n",
        "        self.online_sampling = online_sampling\n",
        "\n",
        "        # compute ratio between HER replays and regular replays in percent for online HER sampling\n",
        "        self.her_ratio = 1 - (1.0 / (self.n_sampled_goal + 1))\n",
        "        # maximum steps in episode\n",
        "        self.max_episode_length = get_time_limit(env, max_episode_length)\n",
        "        # storage for transitions of current episode for offline sampling\n",
        "        # for online sampling, it replaces the \"classic\" replay buffer completely\n",
        "        her_buffer_size = buffer_size if online_sampling else self.max_episode_length\n",
        "\n",
        "        self.env = env\n",
        "        self.buffer_size = her_buffer_size\n",
        "\n",
        "        if online_sampling:\n",
        "            replay_buffer = None\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.online_sampling = online_sampling\n",
        "\n",
        "        # Handle timeouts termination properly if needed\n",
        "        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n",
        "        self.handle_timeout_termination = handle_timeout_termination\n",
        "\n",
        "        # buffer with episodes\n",
        "        # number of episodes which can be stored until buffer size is reached\n",
        "        self.max_episode_stored = self.buffer_size // self.max_episode_length\n",
        "        self.current_idx = 0\n",
        "        # Counter to prevent overflow\n",
        "        self.episode_steps = 0\n",
        "\n",
        "        # Get shape of observation and goal (usually the same)\n",
        "        self.obs_shape = get_obs_shape(self.env.observation_space.spaces[\"observation\"])\n",
        "        self.goal_shape = get_obs_shape(self.env.observation_space.spaces[\"achieved_goal\"])\n",
        "\n",
        "        # input dimensions for buffer initialization\n",
        "        input_shape = {\n",
        "            \"observation\": (self.env.num_envs,) + self.obs_shape,\n",
        "            \"achieved_goal\": (self.env.num_envs,) + self.goal_shape,\n",
        "            \"desired_goal\": (self.env.num_envs,) + self.goal_shape,\n",
        "            \"action\": (self.action_dim,),\n",
        "            \"reward\": (1,),\n",
        "            \"next_obs\": (self.env.num_envs,) + self.obs_shape,\n",
        "            \"next_achieved_goal\": (self.env.num_envs,) + self.goal_shape,\n",
        "            \"next_desired_goal\": (self.env.num_envs,) + self.goal_shape,\n",
        "            \"done\": (1,),\n",
        "        }\n",
        "        self._observation_keys = [\"observation\", \"achieved_goal\", \"desired_goal\"]\n",
        "        self._buffer = {\n",
        "            key: np.zeros((self.max_episode_stored, self.max_episode_length, *dim), dtype=np.float32)\n",
        "            for key, dim in input_shape.items()\n",
        "        }\n",
        "        # Store info dicts are it can be used to compute the reward (e.g. continuity cost)\n",
        "        self.info_buffer = [deque(maxlen=self.max_episode_length) for _ in range(self.max_episode_stored)]\n",
        "        # episode length storage, needed for episodes which has less steps than the maximum length\n",
        "        self.episode_lengths = np.zeros(self.max_episode_stored, dtype=np.int64)\n",
        "\n",
        "    def __getstate__(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Gets state for pickling.\n",
        "        Excludes self.env, as in general Env's may not be pickleable.\n",
        "        Note: when using offline sampling, this will also save the offline replay buffer.\n",
        "        \"\"\"\n",
        "        state = self.__dict__.copy()\n",
        "        # these attributes are not pickleable\n",
        "        del state[\"env\"]\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, state: Dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Restores pickled state.\n",
        "        User must call ``set_env()`` after unpickling before using.\n",
        "        :param state:\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state)\n",
        "        assert \"env\" not in state\n",
        "        self.env = None\n",
        "\n",
        "    def set_env(self, env: VecEnv) -> None:\n",
        "        \"\"\"\n",
        "        Sets the environment.\n",
        "        :param env:\n",
        "        \"\"\"\n",
        "        if self.env is not None:\n",
        "            raise ValueError(\"Trying to set env of already initialized environment.\")\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "    def add(\n",
        "            self,\n",
        "            obs: Dict[str, np.ndarray],\n",
        "            next_obs: Dict[str, np.ndarray],\n",
        "            action: np.ndarray,\n",
        "            reward: np.ndarray,\n",
        "            done: np.ndarray,\n",
        "            infos: List[Dict[str, Any]],\n",
        "    ) -> None:\n",
        "\n",
        "        if self.current_idx == 0 and self.full:\n",
        "            # Clear info buffer\n",
        "            self.info_buffer[self.pos] = deque(maxlen=self.max_episode_length)\n",
        "\n",
        "        # Remove termination signals due to timeout\n",
        "        if self.handle_timeout_termination:\n",
        "            done_ = done * (1 - np.array([info.get(\"TimeLimit.truncated\", False) for info in infos]))\n",
        "        else:\n",
        "            done_ = done\n",
        "\n",
        "        self._buffer[\"observation\"][self.pos][self.current_idx] = obs[\"observation\"]\n",
        "        self._buffer[\"achieved_goal\"][self.pos][self.current_idx] = obs[\"achieved_goal\"]\n",
        "        self._buffer[\"desired_goal\"][self.pos][self.current_idx] = obs[\"desired_goal\"]\n",
        "        self._buffer[\"action\"][self.pos][self.current_idx] = action\n",
        "        self._buffer[\"done\"][self.pos][self.current_idx] = done_\n",
        "        self._buffer[\"reward\"][self.pos][self.current_idx] = reward\n",
        "        self._buffer[\"next_obs\"][self.pos][self.current_idx] = next_obs[\"observation\"]\n",
        "        self._buffer[\"next_achieved_goal\"][self.pos][self.current_idx] = next_obs[\"achieved_goal\"]\n",
        "        self._buffer[\"next_desired_goal\"][self.pos][self.current_idx] = next_obs[\"desired_goal\"]\n",
        "\n",
        "        # When doing offline sampling\n",
        "        # Add real transition to normal replay buffer\n",
        "        if self.replay_buffer is not None:\n",
        "            self.replay_buffer.add(\n",
        "                obs,\n",
        "                next_obs,\n",
        "                action,\n",
        "                reward,\n",
        "                done,\n",
        "                infos,\n",
        "            )\n",
        "\n",
        "        self.info_buffer[self.pos].append(infos)\n",
        "\n",
        "        # update current pointer\n",
        "        self.current_idx += 1\n",
        "\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        if done or self.episode_steps >= self.max_episode_length:\n",
        "            self.store_episode()\n",
        "            if not self.online_sampling:\n",
        "                # sample virtual transitions and store them in replay buffer\n",
        "                self._sample_her_transitions()\n",
        "                # clear storage for current episode\n",
        "                self.reset()\n",
        "\n",
        "            self.episode_steps = 0\n",
        "\n",
        "    def store_episode(self) -> None:\n",
        "        \"\"\"\n",
        "        Increment episode counter\n",
        "        and reset transition pointer.\n",
        "        \"\"\"\n",
        "        # add episode length to length storage\n",
        "        self.episode_lengths[self.pos] = self.current_idx\n",
        "\n",
        "        # update current episode pointer\n",
        "        # Note: in the OpenAI implementation\n",
        "        # when the buffer is full, the episode replaced\n",
        "        # is randomly chosen\n",
        "        self.pos += 1\n",
        "        if self.pos == self.max_episode_stored:\n",
        "            self.full = True\n",
        "            self.pos = 0\n",
        "        # reset transition pointer\n",
        "        self.current_idx = 0\n",
        "\n",
        "    @property\n",
        "    def n_episodes_stored(self) -> int:\n",
        "        if self.full:\n",
        "            return self.max_episode_stored\n",
        "        return self.pos\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"\n",
        "        :return: The current number of transitions in the buffer.\n",
        "        \"\"\"\n",
        "        return int(np.sum(self.episode_lengths))\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"\n",
        "        Reset the buffer.\n",
        "        \"\"\"\n",
        "        self.pos = 0\n",
        "        self.current_idx = 0\n",
        "        self.full = False\n",
        "        self.episode_lengths = np.zeros(self.max_episode_stored, dtype=np.int64)\n",
        "\n",
        "    def truncate_last_trajectory(self) -> None:\n",
        "        \"\"\"\n",
        "        Only for online sampling, called when loading the replay buffer.\n",
        "        If called, we assume that the last trajectory in the replay buffer was finished\n",
        "        (and truncate it).\n",
        "        If not called, we assume that we continue the same trajectory (same episode).\n",
        "        \"\"\"\n",
        "        # If we are at the start of an episode, no need to truncate\n",
        "        current_idx = self.current_idx\n",
        "\n",
        "        # truncate interrupted episode\n",
        "        if current_idx > 0:\n",
        "            warnings.warn(\n",
        "                \"The last trajectory in the replay buffer will be truncated.\\n\"\n",
        "                \"If you are in the same episode as when the replay buffer was saved,\\n\"\n",
        "                \"you should use `truncate_last_trajectory=False` to avoid that issue.\"\n",
        "            )\n",
        "            # get current episode and transition index\n",
        "            pos = self.pos\n",
        "            # set episode length for current episode\n",
        "            self.episode_lengths[pos] = current_idx\n",
        "            # set done = True for current episode\n",
        "            # current_idx was already incremented\n",
        "            self._buffer[\"done\"][pos][current_idx - 1] = np.array([True], dtype=np.float32)\n",
        "            # reset current transition index\n",
        "            self.current_idx = 0\n",
        "            # increment episode counter\n",
        "            self.pos = (self.pos + 1) % self.max_episode_stored\n",
        "            # update \"full\" indicator\n",
        "            self.full = self.full or self.pos == 0\n",
        "\n",
        "\n",
        "class HerReplayBuffer(HerReplayBufferBase):\n",
        "    # Does not require any changes\n",
        "    def __init__(\n",
        "            self,\n",
        "            env: VecEnv,\n",
        "            buffer_size: int,\n",
        "            device: Union[th.device, str] = \"cpu\",\n",
        "            replay_buffer: Optional[DictReplayBufferBase] = None,\n",
        "            max_episode_length: Optional[int] = None,\n",
        "            n_sampled_goal: int = 4,\n",
        "            goal_selection_strategy: str = \"future\",\n",
        "            online_sampling: bool = True,\n",
        "            handle_timeout_termination: bool = True,\n",
        "    ):\n",
        "        super().__init__(env,\n",
        "                         buffer_size,\n",
        "                         device,\n",
        "                         replay_buffer,\n",
        "                         max_episode_length,\n",
        "                         n_sampled_goal,\n",
        "                         goal_selection_strategy,\n",
        "                         online_sampling,\n",
        "                         handle_timeout_termination)\n",
        "\n",
        "    def sample(\n",
        "            self,\n",
        "            batch_size: int,\n",
        "            env: Optional[VecNormalize],\n",
        "    ) -> DictReplayBufferSamples:\n",
        "        \"\"\"\n",
        "        Sample function for online sampling of HER transition,\n",
        "        this replaces the \"regular\" replay buffer ``sample()``\n",
        "        method in the ``train()`` function.\n",
        "        :param batch_size: Number of element to sample\n",
        "        :param env: Associated gym VecEnv\n",
        "            to normalize the observations/rewards when sampling\n",
        "        :return: Samples.\n",
        "        \"\"\"\n",
        "        # TODO: for student\n",
        "        if self.replay_buffer is not None:\n",
        "            return self.replay_buffer.sample(batch_size, env)\n",
        "        return self._sample_transitions(batch_size, maybe_vec_env=env,\n",
        "                                        online_sampling=True)\n",
        "\n",
        "    def _sample_her_transitions(self) -> None:\n",
        "        \"\"\"\n",
        "        Sample additional goals and store new transitions in replay buffer\n",
        "        when using offline sampling.\n",
        "        \"\"\"\n",
        "\n",
        "        # Sample goals to create virtual transitions for the last episode.\n",
        "        # TODO: for student\n",
        "        observations, next_observations, actions, rewards = self._sample_offline(n_sampled_goal=self.n_sampled_goal)\n",
        "\n",
        "        # Store virtual transitions in the replay buffer, if available\n",
        "        # TODO: for student\n",
        "        if len(observations) > 0:\n",
        "            for i in range(len(observations[\"observation\"])):\n",
        "                self.replay_buffer.add(\n",
        "                    {key: obs[i] for key, obs in observations.items()},\n",
        "                    {key: next_obs[i] for key, next_obs in next_observations.items()},\n",
        "                    actions[i],\n",
        "                    rewards[i],\n",
        "                    # We consider the transition as non-terminal\n",
        "                    done=[False],\n",
        "                    infos=[{}],\n",
        "                )\n",
        "\n",
        "    def _sample_offline(\n",
        "            self,\n",
        "            n_sampled_goal: Optional[int] = None,\n",
        "    ) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Sample function for offline sampling of HER transition,\n",
        "        in that case, only one episode is used and transitions\n",
        "        are added to the regular replay buffer.\n",
        "        :param n_sampled_goal: Number of sampled goals for replay\n",
        "        :return: at most(n_sampled_goal * episode_length) HER transitions.\n",
        "        \"\"\"\n",
        "        # `maybe_vec_env=None` as we should store unnormalized transitions,\n",
        "        # they will be normalized at sampling time\n",
        "        return self._sample_transitions(\n",
        "            batch_size=None,\n",
        "            maybe_vec_env=None,\n",
        "            online_sampling=False,\n",
        "            n_sampled_goal=n_sampled_goal,\n",
        "        )\n",
        "\n",
        "    def sample_goals(\n",
        "            self,\n",
        "            episode_indices: np.ndarray,\n",
        "            her_indices: np.ndarray,\n",
        "            transitions_indices: np.ndarray,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample goals based on goal_selection_strategy.\n",
        "        This is a vectorized (fast) version.\n",
        "        :param episode_indices: Episode indices to use.\n",
        "        :param her_indices: HER indices.\n",
        "        :param transitions_indices: Transition indices to use.\n",
        "        :return: Return sampled goals.\n",
        "        \"\"\"\n",
        "        her_episode_indices = episode_indices[her_indices]\n",
        "\n",
        "        if self.goal_selection_strategy == 1:\n",
        "            # replay with final state of current episode\n",
        "            # TODO: for student\n",
        "            transitions_indices = self.episode_lengths[her_episode_indices] - 1\n",
        "\n",
        "        elif self.goal_selection_strategy == 0:\n",
        "            # replay with random state which comes from the same episode and was observed after current transition\n",
        "            # TODO: for student\n",
        "            transitions_indices = np.random.randint(\n",
        "                transitions_indices[her_indices] + 1, self.episode_lengths[her_episode_indices]\n",
        "            )\n",
        "\n",
        "        elif self.goal_selection_strategy == 2:\n",
        "            # replay with random state which comes from the same episode as current transition\n",
        "            # TODO: for student\n",
        "            transitions_indices = np.random.randint(self.episode_lengths[her_episode_indices])\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Strategy {self.goal_selection_strategy} for sampling goals not supported!\")\n",
        "\n",
        "        return self._buffer[\"achieved_goal\"][her_episode_indices, transitions_indices]\n",
        "\n",
        "    def _sample_transitions(\n",
        "            self,\n",
        "            batch_size: Optional[int],\n",
        "            maybe_vec_env: Optional[VecNormalize],\n",
        "            online_sampling: bool,\n",
        "            n_sampled_goal: Optional[int] = None,\n",
        "    ) -> Union[DictReplayBufferSamples, Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        :param batch_size: Number of element to sample (only used for online sampling)\n",
        "        :param env: associated gym VecEnv to normalize the observations/rewards\n",
        "            Only valid when using online sampling\n",
        "        :param online_sampling: Using online_sampling for HER or not.\n",
        "        :param n_sampled_goal: Number of sampled goals for replay. (offline sampling)\n",
        "        :return: Samples.\n",
        "        \"\"\"\n",
        "        # Select which episodes to use\n",
        "        if online_sampling:\n",
        "            assert batch_size is not None, \"No batch_size specified for online sampling of HER transitions\"\n",
        "            # Do not sample the episode with index `self.pos` as the episode is invalid\n",
        "            if self.full:\n",
        "                episode_indices = (\n",
        "                                          np.random.randint(1, self.n_episodes_stored, batch_size) + self.pos\n",
        "                                  ) % self.n_episodes_stored\n",
        "            else:\n",
        "                episode_indices = np.random.randint(0, self.n_episodes_stored, batch_size)\n",
        "            # A subset of the transitions will be relabeled using HER algorithm\n",
        "            her_indices = np.arange(batch_size)[: int(self.her_ratio * batch_size)]\n",
        "        else:\n",
        "            assert maybe_vec_env is None, \"Transitions must be stored unnormalized in the replay buffer\"\n",
        "            assert n_sampled_goal is not None, \"No n_sampled_goal specified for offline sampling of HER transitions\"\n",
        "            # Offline sampling: there is only one episode stored\n",
        "            episode_length = self.episode_lengths[0]\n",
        "            # we sample n_sampled_goal per timestep in the episode (only one is stored).\n",
        "            episode_indices = np.tile(0, (episode_length * n_sampled_goal))\n",
        "            # we only sample virtual transitions\n",
        "            # as real transitions are already stored in the replay buffer\n",
        "            her_indices = np.arange(len(episode_indices))\n",
        "\n",
        "        ep_lengths = self.episode_lengths[episode_indices]\n",
        "\n",
        "        # Special case when using the \"future\" goal sampling strategy\n",
        "        # we cannot sample all transitions, we have to remove the last timestep\n",
        "        if self.goal_selection_strategy == 0:\n",
        "            # restrict the sampling domain when ep_lengths > 1\n",
        "            # otherwise filter out the indices\n",
        "            her_indices = her_indices[ep_lengths[her_indices] > 1]\n",
        "            ep_lengths[her_indices] -= 1\n",
        "\n",
        "        if online_sampling:\n",
        "            # Select which transitions to use\n",
        "            transitions_indices = np.random.randint(ep_lengths)\n",
        "        else:\n",
        "            if her_indices.size == 0:\n",
        "                # Episode of one timestep, not enough for using the \"future\" strategy\n",
        "                # no virtual transitions are created in that case\n",
        "                return {}, {}, np.zeros(0), np.zeros(0)\n",
        "            else:\n",
        "                # Repeat every transition index n_sampled_goals times\n",
        "                # to sample n_sampled_goal per timestep in the episode (only one is stored).\n",
        "                # Now with the corrected episode length when using \"future\" strategy\n",
        "                transitions_indices = np.tile(np.arange(ep_lengths[0]), n_sampled_goal)\n",
        "                episode_indices = episode_indices[transitions_indices]\n",
        "                her_indices = np.arange(len(episode_indices))\n",
        "\n",
        "        # get selected transitions\n",
        "        transitions = {key: self._buffer[key][episode_indices, transitions_indices].copy() for key in\n",
        "                       self._buffer.keys()}\n",
        "\n",
        "        # sample new desired goals and relabel the transitions\n",
        "        new_goals = self.sample_goals(episode_indices, her_indices, transitions_indices)\n",
        "        transitions[\"desired_goal\"][her_indices] = new_goals\n",
        "\n",
        "        # Convert info buffer to numpy array\n",
        "        transitions[\"info\"] = np.array(\n",
        "            [\n",
        "                self.info_buffer[episode_idx][transition_idx]\n",
        "                for episode_idx, transition_idx in zip(episode_indices, transitions_indices)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Edge case: episode of one timesteps with the future strategy\n",
        "        # no virtual transition can be created\n",
        "        if len(her_indices) > 0:\n",
        "            # Vectorized computation of the new reward\n",
        "            transitions[\"reward\"][her_indices, 0] = self.env.env_method(\n",
        "                \"compute_reward\",\n",
        "                # the new state depends on the previous state and action\n",
        "                # s_{t+1} = f(s_t, a_t)\n",
        "                # so the next_achieved_goal depends also on the previous state and action\n",
        "                # because we are in a GoalEnv:\n",
        "                # r_t = reward(s_t, a_t) = reward(next_achieved_goal, desired_goal)\n",
        "                # therefore we have to use \"next_achieved_goal\" and not \"achieved_goal\"\n",
        "                transitions[\"next_achieved_goal\"][her_indices, 0],\n",
        "                # here we use the new desired goal\n",
        "                transitions[\"desired_goal\"][her_indices, 0],\n",
        "                transitions[\"info\"][her_indices, 0],\n",
        "            )\n",
        "\n",
        "        # concatenate observation with (desired) goal\n",
        "        observations = self._normalize_obs(transitions, maybe_vec_env)\n",
        "\n",
        "        # HACK to make normalize obs and `add()` work with the next observation\n",
        "        next_observations = {\n",
        "            \"observation\": transitions[\"next_obs\"],\n",
        "            \"achieved_goal\": transitions[\"next_achieved_goal\"],\n",
        "            # The desired goal for the next observation must be the same as the previous one\n",
        "            \"desired_goal\": transitions[\"desired_goal\"],\n",
        "        }\n",
        "        next_observations = self._normalize_obs(next_observations, maybe_vec_env)\n",
        "\n",
        "        if online_sampling:\n",
        "            next_obs = {key: self.to_torch(next_observations[key][:, 0, :]) for key in self._observation_keys}\n",
        "\n",
        "            normalized_obs = {key: self.to_torch(observations[key][:, 0, :]) for key in self._observation_keys}\n",
        "\n",
        "            return DictReplayBufferSamples(\n",
        "                observations=normalized_obs,\n",
        "                actions=self.to_torch(transitions[\"action\"]),\n",
        "                next_observations=next_obs,\n",
        "                dones=self.to_torch(transitions[\"done\"]),\n",
        "                rewards=self.to_torch(self._normalize_reward(transitions[\"reward\"], maybe_vec_env)),\n",
        "            )\n",
        "        else:\n",
        "            return observations, next_observations, transitions[\"action\"], transitions[\"reward\"]\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWdgvi_Qq92e"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCrsgsJ6rMlm"
      },
      "source": [
        "### Off-policy algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_D3Z7D_q_qZ"
      },
      "source": [
        "import io\n",
        "import pathlib\n",
        "import time\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Optional, Tuple, Type, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.noise import ActionNoise\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, RolloutReturn, Schedule, TrainFreq, TrainFrequencyUnit\n",
        "from stable_baselines3.common.utils import safe_mean, should_collect_more_steps\n",
        "from stable_baselines3.common.vec_env import VecEnv\n",
        "\n",
        "\n",
        "class OffPolicyAlgorithm(BaseAlgorithm):\n",
        "    \"\"\"\n",
        "    The base for Off-Policy algorithms (ex: SAC/TD3)\n",
        "    :param policy: Policy object\n",
        "    :param env: The environment to learn from\n",
        "                (if registered in Gym, can be str. Can be None for loading trained models)\n",
        "    :param policy_base: The base policy used by this method\n",
        "    :param learning_rate: learning rate for the optimizer,\n",
        "        it can be a function of the current progress remaining (from 1 to 0)\n",
        "    :param buffer_size: size of the replay buffer\n",
        "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
        "    :param gamma: the discount factor\n",
        "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
        "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
        "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
        "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
        "        during the rollout.\n",
        "    :param action_noise: the action noise type (None by default), this can help\n",
        "        for hard exploration problem. Cf common.noise for the different action noise type.\n",
        "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
        "        If ``None``, it will be automatically selected.\n",
        "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
        "        at a cost of more complexity.\n",
        "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
        "    :param policy_kwargs: Additional arguments to be passed to the policy on creation\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param verbose: The verbosity level: 0 none, 1 training information, 2 debug\n",
        "    :param device: Device on which the code should run.\n",
        "        By default, it will try to use a Cuda compatible device and fallback to cpu\n",
        "        if it is not possible.\n",
        "    :param support_multi_env: Whether the algorithm supports training\n",
        "        with multiple environments (as in A2C)\n",
        "    :param create_eval_env: Whether to create a second environment that will be\n",
        "        used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
        "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
        "        or not in a Monitor wrapper.\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param use_sde: Whether to use State Dependent Exploration (SDE)\n",
        "        instead of action noise exploration (default: False)\n",
        "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
        "        Default: -1 (only sample at the beginning of the rollout)\n",
        "    :param use_sde_at_warmup: Whether to use gSDE instead of uniform sampling\n",
        "        during the warm up phase (before learning starts)\n",
        "    :param sde_support: Whether the model support gSDE or not\n",
        "    :param remove_time_limit_termination: Remove terminations (dones) that are due to time limit.\n",
        "        See https://github.com/hill-a/stable-baselines/issues/863\n",
        "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Type[BasePolicy],\n",
        "        env: Union[GymEnv, str],\n",
        "        policy_base: Type[BasePolicy],\n",
        "        learning_rate: Union[float, Schedule],\n",
        "        buffer_size: int = 1000000,  # 1e6\n",
        "        learning_starts: int = 100,\n",
        "        batch_size: int = 256,\n",
        "        tau: float = 0.005,\n",
        "        gamma: float = 0.99,\n",
        "        train_freq: Union[int, Tuple[int, str]] = (1, \"step\"),\n",
        "        gradient_steps: int = 1,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        replay_buffer_class: Optional[ReplayBuffer] = None,\n",
        "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        optimize_memory_usage: bool = False,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        verbose: int = 0,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        support_multi_env: bool = False,\n",
        "        create_eval_env: bool = False,\n",
        "        monitor_wrapper: bool = True,\n",
        "        seed: Optional[int] = None,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        use_sde_at_warmup: bool = False,\n",
        "        sde_support: bool = True,\n",
        "        remove_time_limit_termination: bool = False,\n",
        "        supported_action_spaces: Optional[Tuple[gym.spaces.Space, ...]] = None,\n",
        "    ):\n",
        "\n",
        "        super(OffPolicyAlgorithm, self).__init__(\n",
        "            policy=policy,\n",
        "            env=env,\n",
        "            policy_base=policy_base,\n",
        "            learning_rate=learning_rate,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            support_multi_env=support_multi_env,\n",
        "            create_eval_env=create_eval_env,\n",
        "            monitor_wrapper=monitor_wrapper,\n",
        "            seed=seed,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            supported_action_spaces=supported_action_spaces,\n",
        "        )\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_starts = learning_starts\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.gradient_steps = gradient_steps\n",
        "        self.action_noise = action_noise\n",
        "        self.optimize_memory_usage = optimize_memory_usage\n",
        "        self.replay_buffer_class = replay_buffer_class\n",
        "        if replay_buffer_kwargs is None:\n",
        "            replay_buffer_kwargs = {}\n",
        "        self.replay_buffer_kwargs = replay_buffer_kwargs\n",
        "        self._episode_storage = None\n",
        "\n",
        "        # Remove terminations (dones) that are due to time limit\n",
        "        # see https://github.com/hill-a/stable-baselines/issues/863\n",
        "        self.remove_time_limit_termination = remove_time_limit_termination\n",
        "\n",
        "        # Save train freq parameter, will be converted later to TrainFreq object\n",
        "        self.train_freq = train_freq\n",
        "\n",
        "        self.actor = None  # type: Optional[th.nn.Module]\n",
        "        self.replay_buffer = None  # type: Optional[ReplayBuffer]\n",
        "        # Update policy keyword arguments\n",
        "        if sde_support:\n",
        "            self.policy_kwargs[\"use_sde\"] = self.use_sde\n",
        "        # For gSDE only\n",
        "        self.use_sde_at_warmup = use_sde_at_warmup\n",
        "\n",
        "    def _convert_train_freq(self) -> None:\n",
        "        \"\"\"\n",
        "        Convert `train_freq` parameter (int or tuple)\n",
        "        to a TrainFreq object.\n",
        "        \"\"\"\n",
        "        if not isinstance(self.train_freq, TrainFreq):\n",
        "            train_freq = self.train_freq\n",
        "\n",
        "            # The value of the train frequency will be checked later\n",
        "            if not isinstance(train_freq, tuple):\n",
        "                train_freq = (train_freq, \"step\")\n",
        "\n",
        "            try:\n",
        "                train_freq = (train_freq[0], TrainFrequencyUnit(train_freq[1]))\n",
        "            except ValueError:\n",
        "                raise ValueError(f\"The unit of the `train_freq` must be either 'step' or 'episode' not '{train_freq[1]}'!\")\n",
        "\n",
        "            if not isinstance(train_freq[0], int):\n",
        "                raise ValueError(f\"The frequency of `train_freq` must be an integer and not {train_freq[0]}\")\n",
        "\n",
        "            self.train_freq = TrainFreq(*train_freq)\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        self._setup_lr_schedule()\n",
        "        self.set_random_seed(self.seed)\n",
        "\n",
        "        # Use DictReplayBuffer if needed\n",
        "        if self.replay_buffer_class is None:\n",
        "            if isinstance(self.observation_space, gym.spaces.Dict):\n",
        "                self.replay_buffer_class = DictReplayBuffer\n",
        "            else:\n",
        "                self.replay_buffer_class = ReplayBuffer\n",
        "\n",
        "        elif self.replay_buffer_class == HerReplayBuffer:\n",
        "            assert self.env is not None, \"You must pass an environment when using `HerReplayBuffer`\"\n",
        "\n",
        "            # If using offline sampling, we need a classic replay buffer too\n",
        "            if self.replay_buffer_kwargs.get(\"online_sampling\", True):\n",
        "                replay_buffer = None\n",
        "            else:\n",
        "                replay_buffer = DictReplayBuffer(\n",
        "                    self.buffer_size,\n",
        "                    self.observation_space,\n",
        "                    self.action_space,\n",
        "                    self.device,\n",
        "                    optimize_memory_usage=self.optimize_memory_usage,\n",
        "                )\n",
        "\n",
        "            self.replay_buffer = HerReplayBuffer(\n",
        "                self.env,\n",
        "                self.buffer_size,\n",
        "                self.device,\n",
        "                replay_buffer=replay_buffer,\n",
        "                **self.replay_buffer_kwargs,\n",
        "            )\n",
        "\n",
        "        if self.replay_buffer is None:\n",
        "            self.replay_buffer = self.replay_buffer_class(\n",
        "                self.buffer_size,\n",
        "                self.observation_space,\n",
        "                self.action_space,\n",
        "                self.device,\n",
        "                optimize_memory_usage=self.optimize_memory_usage,\n",
        "                **self.replay_buffer_kwargs,\n",
        "            )\n",
        "\n",
        "        self.policy = self.policy_class(  # pytype:disable=not-instantiable\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            self.lr_schedule,\n",
        "            **self.policy_kwargs,  # pytype:disable=not-instantiable\n",
        "        )\n",
        "        self.policy = self.policy.to(self.device)\n",
        "\n",
        "        # Convert train freq parameter to TrainFreq object\n",
        "        self._convert_train_freq()\n",
        "\n",
        "    def save_replay_buffer(self, path: Union[str, pathlib.Path, io.BufferedIOBase]) -> None:\n",
        "        \"\"\"\n",
        "        Save the replay buffer as a pickle file.\n",
        "        :param path: Path to the file where the replay buffer should be saved.\n",
        "            if path is a str or pathlib.Path, the path is automatically created if necessary.\n",
        "        \"\"\"\n",
        "        assert self.replay_buffer is not None, \"The replay buffer is not defined\"\n",
        "        save_to_pkl(path, self.replay_buffer, self.verbose)\n",
        "\n",
        "    def load_replay_buffer(\n",
        "        self,\n",
        "        path: Union[str, pathlib.Path, io.BufferedIOBase],\n",
        "        truncate_last_traj: bool = True,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Load a replay buffer from a pickle file.\n",
        "        :param path: Path to the pickled replay buffer.\n",
        "        :param truncate_last_traj: When using ``HerReplayBuffer`` with online sampling:\n",
        "            If set to ``True``, we assume that the last trajectory in the replay buffer was finished\n",
        "            (and truncate it).\n",
        "            If set to ``False``, we assume that we continue the same trajectory (same episode).\n",
        "        \"\"\"\n",
        "        self.replay_buffer = load_from_pkl(path, self.verbose)\n",
        "        assert isinstance(self.replay_buffer, ReplayBuffer), \"The replay buffer must inherit from ReplayBuffer class\"\n",
        "\n",
        "        # Backward compatibility with SB3 < 2.1.0 replay buffer\n",
        "        # Keep old behavior: do not handle timeout termination separately\n",
        "        if not hasattr(self.replay_buffer, \"handle_timeout_termination\"):  # pragma: no cover\n",
        "            self.replay_buffer.handle_timeout_termination = False\n",
        "            self.replay_buffer.timeouts = np.zeros_like(self.replay_buffer.dones)\n",
        "\n",
        "        if isinstance(self.replay_buffer, HerReplayBuffer):\n",
        "            assert self.env is not None, \"You must pass an environment at load time when using `HerReplayBuffer`\"\n",
        "            self.replay_buffer.set_env(self.get_env())\n",
        "            if truncate_last_traj:\n",
        "                self.replay_buffer.truncate_last_trajectory()\n",
        "\n",
        "    def _setup_learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        eval_env: Optional[GymEnv],\n",
        "        callback: MaybeCallback = None,\n",
        "        eval_freq: int = 10000,\n",
        "        n_eval_episodes: int = 5,\n",
        "        log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "        tb_log_name: str = \"run\",\n",
        "    ) -> Tuple[int, BaseCallback]:\n",
        "        \"\"\"\n",
        "        cf `BaseAlgorithm`.\n",
        "        \"\"\"\n",
        "        # Prevent continuity issue by truncating trajectory\n",
        "        # when using memory efficient replay buffer\n",
        "        # see https://github.com/DLR-RM/stable-baselines3/issues/46\n",
        "\n",
        "        # Special case when using HerReplayBuffer,\n",
        "        # the classic replay buffer is inside it when using offline sampling\n",
        "        if isinstance(self.replay_buffer, HerReplayBuffer):\n",
        "            replay_buffer = self.replay_buffer.replay_buffer\n",
        "        else:\n",
        "            replay_buffer = self.replay_buffer\n",
        "\n",
        "        truncate_last_traj = (\n",
        "            self.optimize_memory_usage\n",
        "            and reset_num_timesteps\n",
        "            and replay_buffer is not None\n",
        "            and (replay_buffer.full or replay_buffer.pos > 0)\n",
        "        )\n",
        "\n",
        "        if truncate_last_traj:\n",
        "            warnings.warn(\n",
        "                \"The last trajectory in the replay buffer will be truncated, \"\n",
        "                \"see https://github.com/DLR-RM/stable-baselines3/issues/46.\"\n",
        "                \"You should use `reset_num_timesteps=False` or `optimize_memory_usage=False`\"\n",
        "                \"to avoid that issue.\"\n",
        "            )\n",
        "            # Go to the previous index\n",
        "            pos = (replay_buffer.pos - 1) % replay_buffer.buffer_size\n",
        "            replay_buffer.dones[pos] = True\n",
        "\n",
        "        return super()._setup_learn(\n",
        "            total_timesteps,\n",
        "            eval_env,\n",
        "            callback,\n",
        "            eval_freq,\n",
        "            n_eval_episodes,\n",
        "            log_path,\n",
        "            reset_num_timesteps,\n",
        "            tb_log_name,\n",
        "        )\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 4,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"run\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "    ) -> \"OffPolicyAlgorithm\":\n",
        "\n",
        "        total_timesteps, callback = self._setup_learn(\n",
        "            total_timesteps,\n",
        "            eval_env,\n",
        "            callback,\n",
        "            eval_freq,\n",
        "            n_eval_episodes,\n",
        "            eval_log_path,\n",
        "            reset_num_timesteps,\n",
        "            tb_log_name,\n",
        "        )\n",
        "\n",
        "        callback.on_training_start(locals(), globals())\n",
        "\n",
        "        while self.num_timesteps < total_timesteps:\n",
        "            rollout = self.collect_rollouts(\n",
        "                self.env,\n",
        "                train_freq=self.train_freq,\n",
        "                action_noise=self.action_noise,\n",
        "                callback=callback,\n",
        "                learning_starts=self.learning_starts,\n",
        "                replay_buffer=self.replay_buffer,\n",
        "                log_interval=log_interval,\n",
        "            )\n",
        "\n",
        "            if rollout.continue_training is False:\n",
        "                break\n",
        "\n",
        "            if self.num_timesteps > 0 and self.num_timesteps > self.learning_starts:\n",
        "                # If no `gradient_steps` is specified,\n",
        "                # do as many gradients steps as steps performed during the rollout\n",
        "                gradient_steps = self.gradient_steps if self.gradient_steps >= 0 else rollout.episode_timesteps\n",
        "                # Special case when the user passes `gradient_steps=0`\n",
        "                if gradient_steps > 0:\n",
        "                    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)\n",
        "\n",
        "        callback.on_training_end()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train(self, gradient_steps: int, batch_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Sample the replay buffer and do the updates\n",
        "        (gradient descent and update target networks)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _sample_action(\n",
        "        self, learning_starts: int, action_noise: Optional[ActionNoise] = None\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Sample an action according to the exploration policy.\n",
        "        This is either done by sampling the probability distribution of the policy,\n",
        "        or sampling a random action (from a uniform distribution over the action space)\n",
        "        or by adding noise to the deterministic output.\n",
        "        :param action_noise: Action noise that will be used for exploration\n",
        "            Required for deterministic policy (e.g. TD3). This can also be used\n",
        "            in addition to the stochastic policy for SAC.\n",
        "        :param learning_starts: Number of steps before learning for the warm-up phase.\n",
        "        :return: action to take in the environment\n",
        "            and scaled action that will be stored in the replay buffer.\n",
        "            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n",
        "        \"\"\"\n",
        "        # Select action randomly or according to policy\n",
        "        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n",
        "            # Warmup phase\n",
        "            unscaled_action = np.array([self.action_space.sample()])\n",
        "        else:\n",
        "            # Note: when using continuous actions,\n",
        "            # we assume that the policy uses tanh to scale the action\n",
        "            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n",
        "            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n",
        "\n",
        "        # Rescale the action from [low, high] to [-1, 1]\n",
        "        if isinstance(self.action_space, gym.spaces.Box):\n",
        "            scaled_action = self.policy.scale_action(unscaled_action)\n",
        "\n",
        "            # Add noise to the action (improve exploration)\n",
        "            if action_noise is not None:\n",
        "                scaled_action = np.clip(scaled_action + action_noise(), -1, 1)\n",
        "\n",
        "            # We store the scaled action in the buffer\n",
        "            buffer_action = scaled_action\n",
        "            action = self.policy.unscale_action(scaled_action)\n",
        "        else:\n",
        "            # Discrete case, no need to normalize or clip\n",
        "            buffer_action = unscaled_action\n",
        "            action = buffer_action\n",
        "        return action, buffer_action\n",
        "\n",
        "    def _dump_logs(self) -> None:\n",
        "        \"\"\"\n",
        "        Write log.\n",
        "        \"\"\"\n",
        "        time_elapsed = time.time() - self.start_time\n",
        "        fps = int(self.num_timesteps / (time_elapsed + 1e-8))\n",
        "        self.logger.record(\"time/episodes\", self._episode_num, exclude=\"tensorboard\")\n",
        "        if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
        "            self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
        "            self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
        "        self.logger.record(\"time/fps\", fps)\n",
        "        self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
        "        self.logger.record(\"time/total timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
        "        if self.use_sde:\n",
        "            self.logger.record(\"train/std\", (self.actor.get_std()).mean().item())\n",
        "\n",
        "        if len(self.ep_success_buffer) > 0:\n",
        "            self.logger.record(\"rollout/success rate\", safe_mean(self.ep_success_buffer))\n",
        "        # Pass the number of timesteps for tensorboard\n",
        "        self.logger.dump(step=self.num_timesteps)\n",
        "\n",
        "    def _on_step(self) -> None:\n",
        "        \"\"\"\n",
        "        Method called after each step in the environment.\n",
        "        It is meant to trigger DQN target network update\n",
        "        but can be used for other purposes\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _store_transition(\n",
        "        self,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        buffer_action: np.ndarray,\n",
        "        new_obs: np.ndarray,\n",
        "        reward: np.ndarray,\n",
        "        done: np.ndarray,\n",
        "        infos: List[Dict[str, Any]],\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Store transition in the replay buffer.\n",
        "        We store the normalized action and the unnormalized observation.\n",
        "        It also handles terminal observations (because VecEnv resets automatically).\n",
        "        :param replay_buffer: Replay buffer object where to store the transition.\n",
        "        :param buffer_action: normalized action\n",
        "        :param new_obs: next observation in the current episode\n",
        "            or first observation of the episode (when done is True)\n",
        "        :param reward: reward for the current transition\n",
        "        :param done: Termination signal\n",
        "        :param infos: List of additional information about the transition.\n",
        "            It may contain the terminal observations and information about timeout.\n",
        "        \"\"\"\n",
        "        # Store only the unnormalized version\n",
        "        if self._vec_normalize_env is not None:\n",
        "            new_obs_ = self._vec_normalize_env.get_original_obs()\n",
        "            reward_ = self._vec_normalize_env.get_original_reward()\n",
        "        else:\n",
        "            # Avoid changing the original ones\n",
        "            self._last_original_obs, new_obs_, reward_ = self._last_obs, new_obs, reward\n",
        "\n",
        "        # As the VecEnv resets automatically, new_obs is already the\n",
        "        # first observation of the next episode\n",
        "        if done and infos[0].get(\"terminal_observation\") is not None:\n",
        "            next_obs = infos[0][\"terminal_observation\"]\n",
        "            # VecNormalize normalizes the terminal observation\n",
        "            if self._vec_normalize_env is not None:\n",
        "                next_obs = self._vec_normalize_env.unnormalize_obs(next_obs)\n",
        "        else:\n",
        "            next_obs = new_obs_\n",
        "\n",
        "        replay_buffer.add(\n",
        "            self._last_original_obs,\n",
        "            next_obs,\n",
        "            buffer_action,\n",
        "            reward_,\n",
        "            done,\n",
        "            infos,\n",
        "        )\n",
        "\n",
        "        self._last_obs = new_obs\n",
        "        # Save the unnormalized observation\n",
        "        if self._vec_normalize_env is not None:\n",
        "            self._last_original_obs = new_obs_\n",
        "\n",
        "    def collect_rollouts(\n",
        "        self,\n",
        "        env: VecEnv,\n",
        "        callback: BaseCallback,\n",
        "        train_freq: TrainFreq,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        learning_starts: int = 0,\n",
        "        log_interval: Optional[int] = None,\n",
        "    ) -> RolloutReturn:\n",
        "        \"\"\"\n",
        "        Collect experiences and store them into a ``ReplayBuffer``.\n",
        "        :param env: The training environment\n",
        "        :param callback: Callback that will be called at each step\n",
        "            (and at the beginning and end of the rollout)\n",
        "        :param train_freq: How much experience to collect\n",
        "            by doing rollouts of current policy.\n",
        "            Either ``TrainFreq(<n>, TrainFrequencyUnit.STEP)``\n",
        "            or ``TrainFreq(<n>, TrainFrequencyUnit.EPISODE)``\n",
        "            with ``<n>`` being an integer greater than 0.\n",
        "        :param action_noise: Action noise that will be used for exploration\n",
        "            Required for deterministic policy (e.g. TD3). This can also be used\n",
        "            in addition to the stochastic policy for SAC.\n",
        "        :param learning_starts: Number of steps before learning for the warm-up phase.\n",
        "        :param replay_buffer:\n",
        "        :param log_interval: Log data every ``log_interval`` episodes\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Switch to eval mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(False)\n",
        "\n",
        "        episode_rewards, total_timesteps = [], []\n",
        "        num_collected_steps, num_collected_episodes = 0, 0\n",
        "\n",
        "        assert isinstance(env, VecEnv), \"You must pass a VecEnv\"\n",
        "        assert env.num_envs == 1, \"OffPolicyAlgorithm only support single environment\"\n",
        "        assert train_freq.frequency > 0, \"Should at least collect one step or episode.\"\n",
        "\n",
        "        if self.use_sde:\n",
        "            self.actor.reset_noise()\n",
        "\n",
        "        callback.on_rollout_start()\n",
        "        continue_training = True\n",
        "\n",
        "        while should_collect_more_steps(train_freq, num_collected_steps, num_collected_episodes):\n",
        "            done = False\n",
        "            episode_reward, episode_timesteps = 0.0, 0\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                if self.use_sde and self.sde_sample_freq > 0 and num_collected_steps % self.sde_sample_freq == 0:\n",
        "                    # Sample a new noise matrix\n",
        "                    self.actor.reset_noise()\n",
        "\n",
        "                # Select action randomly or according to policy\n",
        "                action, buffer_action = self._sample_action(learning_starts, action_noise)\n",
        "\n",
        "                # Rescale and perform action\n",
        "                new_obs, reward, done, infos = env.step(action)\n",
        "\n",
        "                self.num_timesteps += 1\n",
        "                episode_timesteps += 1\n",
        "                num_collected_steps += 1\n",
        "\n",
        "                # Give access to local variables\n",
        "                callback.update_locals(locals())\n",
        "                # Only stop training if return value is False, not when it is None.\n",
        "                if callback.on_step() is False:\n",
        "                    return RolloutReturn(0.0, num_collected_steps, num_collected_episodes, continue_training=False)\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Retrieve reward and episode length if using Monitor wrapper\n",
        "                self._update_info_buffer(infos, done)\n",
        "\n",
        "                # Store data in replay buffer (normalized action and unnormalized observation)\n",
        "                self._store_transition(replay_buffer, buffer_action, new_obs, reward, done, infos)\n",
        "\n",
        "                self._update_current_progress_remaining(self.num_timesteps, self._total_timesteps)\n",
        "\n",
        "                # For DQN, check if the target network should be updated\n",
        "                # and update the exploration schedule\n",
        "                # For SAC/TD3, the update is done as the same time as the gradient update\n",
        "                # see https://github.com/hill-a/stable-baselines/issues/900\n",
        "                self._on_step()\n",
        "\n",
        "                if not should_collect_more_steps(train_freq, num_collected_steps, num_collected_episodes):\n",
        "                    break\n",
        "\n",
        "            if done:\n",
        "                num_collected_episodes += 1\n",
        "                self._episode_num += 1\n",
        "                episode_rewards.append(episode_reward)\n",
        "                total_timesteps.append(episode_timesteps)\n",
        "\n",
        "                if action_noise is not None:\n",
        "                    action_noise.reset()\n",
        "\n",
        "                # Log training infos\n",
        "                if log_interval is not None and self._episode_num % log_interval == 0:\n",
        "                    self._dump_logs()\n",
        "\n",
        "        mean_reward = np.mean(episode_rewards) if num_collected_episodes > 0 else 0.0\n",
        "\n",
        "        callback.on_rollout_end()\n",
        "\n",
        "        return RolloutReturn(mean_reward, num_collected_steps, num_collected_episodes, continue_training)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNxjYAVHrRQx"
      },
      "source": [
        "### TD3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9sySpWarS3A"
      },
      "source": [
        "from typing import Any, Dict, List, Optional, Tuple, Type, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from stable_baselines3.common.noise import ActionNoise\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import polyak_update\n",
        "from stable_baselines3.td3.policies import TD3Policy\n",
        "\n",
        "\n",
        "class TD3(OffPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Twin Delayed DDPG (TD3)\n",
        "    Addressing Function Approximation Error in Actor-Critic Methods.\n",
        "    Original implementation: https://github.com/sfujim/TD3\n",
        "    Paper: https://arxiv.org/abs/1802.09477\n",
        "    Introduction to TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: learning rate for adam optimizer,\n",
        "        the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n",
        "        it can be a function of the current progress remaining (from 1 to 0)\n",
        "    :param buffer_size: size of the replay buffer\n",
        "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
        "    :param gamma: the discount factor\n",
        "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
        "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
        "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
        "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
        "        during the rollout.\n",
        "    :param action_noise: the action noise type (None by default), this can help\n",
        "        for hard exploration problem. Cf common.noise for the different action noise type.\n",
        "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
        "        If ``None``, it will be automatically selected.\n",
        "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
        "        at a cost of more complexity.\n",
        "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
        "    :param policy_delay: Policy and target networks will only be updated once every policy_delay steps\n",
        "        per training steps. The Q values will be updated policy_delay more often (update every training step).\n",
        "    :param target_policy_noise: Standard deviation of Gaussian noise added to target policy\n",
        "        (smoothing noise)\n",
        "    :param target_noise_clip: Limit for absolute value of target policy smoothing noise.\n",
        "    :param create_eval_env: Whether to create a second environment that will be\n",
        "        used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[TD3Policy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 1e-3,\n",
        "        buffer_size: int = 1000000,  # 1e6\n",
        "        learning_starts: int = 100,\n",
        "        batch_size: int = 100,\n",
        "        tau: float = 0.005,\n",
        "        gamma: float = 0.99,\n",
        "        train_freq: Union[int, Tuple[int, str]] = (1, \"episode\"),\n",
        "        gradient_steps: int = -1,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        replay_buffer_class: Optional[ReplayBuffer] = None,\n",
        "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        optimize_memory_usage: bool = False,\n",
        "        policy_delay: int = 2,\n",
        "        target_policy_noise: float = 0.2,\n",
        "        target_noise_clip: float = 0.5,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        create_eval_env: bool = False,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "\n",
        "        super(TD3, self).__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            TD3Policy,\n",
        "            learning_rate,\n",
        "            buffer_size,\n",
        "            learning_starts,\n",
        "            batch_size,\n",
        "            tau,\n",
        "            gamma,\n",
        "            train_freq,\n",
        "            gradient_steps,\n",
        "            action_noise=action_noise,\n",
        "            replay_buffer_class=replay_buffer_class,\n",
        "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            create_eval_env=create_eval_env,\n",
        "            seed=seed,\n",
        "            sde_support=False,\n",
        "            optimize_memory_usage=optimize_memory_usage,\n",
        "            supported_action_spaces=(gym.spaces.Box),\n",
        "        )\n",
        "\n",
        "        self.policy_delay = policy_delay\n",
        "        self.target_noise_clip = target_noise_clip\n",
        "        self.target_policy_noise = target_policy_noise\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        super(TD3, self)._setup_model()\n",
        "        self._create_aliases()\n",
        "\n",
        "    def _create_aliases(self) -> None:\n",
        "        self.actor = self.policy.actor\n",
        "        self.actor_target = self.policy.actor_target\n",
        "        self.critic = self.policy.critic\n",
        "        self.critic_target = self.policy.critic_target\n",
        "\n",
        "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "\n",
        "        # Update learning rate according to lr schedule\n",
        "        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n",
        "\n",
        "        actor_losses, critic_losses = [], []\n",
        "\n",
        "        for _ in range(gradient_steps):\n",
        "\n",
        "            self._n_updates += 1\n",
        "            # Sample replay buffer\n",
        "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
        "\n",
        "            with th.no_grad():\n",
        "                # Select action according to policy and add clipped noise\n",
        "                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n",
        "                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n",
        "                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n",
        "\n",
        "                # Compute the next Q-values: min over all critics targets\n",
        "                next_q_values = th.cat(self.critic_target(replay_data.next_observations, next_actions), dim=1)\n",
        "                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
        "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
        "\n",
        "            # Get current Q-values estimates for each critic network\n",
        "            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = sum([F.mse_loss(current_q, target_q_values) for current_q in current_q_values])\n",
        "            critic_losses.append(critic_loss.item())\n",
        "\n",
        "            # Optimize the critics\n",
        "            self.critic.optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic.optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if self._n_updates % self.policy_delay == 0:\n",
        "                # Compute actor loss\n",
        "                actor_loss = -self.critic.q1_forward(replay_data.observations, self.actor(replay_data.observations)).mean()\n",
        "                actor_losses.append(actor_loss.item())\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "\n",
        "                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n",
        "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        if len(actor_losses) > 0:\n",
        "            self.logger.record(\"train/actor_loss\", np.mean(actor_losses))\n",
        "        self.logger.record(\"train/critic_loss\", np.mean(critic_losses))\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 4,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"TD3\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "    ) -> OffPolicyAlgorithm:\n",
        "\n",
        "        return super(TD3, self).learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            eval_env=eval_env,\n",
        "            eval_freq=eval_freq,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            tb_log_name=tb_log_name,\n",
        "            eval_log_path=eval_log_path,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "        )\n",
        "\n",
        "    def _excluded_save_params(self) -> List[str]:\n",
        "        return super(TD3, self)._excluded_save_params() + [\"actor\", \"critic\", \"actor_target\", \"critic_target\"]\n",
        "\n",
        "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
        "        state_dicts = [\"policy\", \"actor.optimizer\", \"critic.optimizer\"]\n",
        "        return state_dicts, []"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK4YTm-prWpm"
      },
      "source": [
        "### DDPG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enDjGBJdrqMp"
      },
      "source": [
        "from typing import Any, Dict, Optional, Tuple, Type, Union\n",
        "\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3.common.noise import ActionNoise\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "\n",
        "\n",
        "class DDPG(TD3):\n",
        "    \"\"\"\n",
        "    Deep Deterministic Policy Gradient (DDPG).\n",
        "    Deterministic Policy Gradient: http://proceedings.mlr.press/v32/silver14.pdf\n",
        "    DDPG Paper: https://arxiv.org/abs/1509.02971\n",
        "    Introduction to DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
        "    Note: we treat DDPG as a special case of its successor TD3.\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: learning rate for adam optimizer,\n",
        "        the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n",
        "        it can be a function of the current progress remaining (from 1 to 0)\n",
        "    :param buffer_size: size of the replay buffer\n",
        "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
        "    :param gamma: the discount factor\n",
        "    :param train_freq: Update the model every ``train_freq`` steps. Alternatively pass a tuple of frequency and unit\n",
        "        like ``(5, \"step\")`` or ``(2, \"episode\")``.\n",
        "    :param gradient_steps: How many gradient steps to do after each rollout (see ``train_freq``)\n",
        "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
        "        during the rollout.\n",
        "    :param action_noise: the action noise type (None by default), this can help\n",
        "        for hard exploration problem. Cf common.noise for the different action noise type.\n",
        "    :param replay_buffer_class: Replay buffer class to use (for instance ``HerReplayBuffer``).\n",
        "        If ``None``, it will be automatically selected.\n",
        "    :param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
        "        at a cost of more complexity.\n",
        "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
        "    :param create_eval_env: Whether to create a second environment that will be\n",
        "        used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[TD3Policy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 1e-3,\n",
        "        buffer_size: int = 1000000,  # 1e6\n",
        "        learning_starts: int = 100,\n",
        "        batch_size: int = 100,\n",
        "        tau: float = 0.005,\n",
        "        gamma: float = 0.99,\n",
        "        train_freq: Union[int, Tuple[int, str]] = (1, \"episode\"),\n",
        "        gradient_steps: int = -1,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        replay_buffer_class: Optional[ReplayBuffer] = None,\n",
        "        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        optimize_memory_usage: bool = False,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        create_eval_env: bool = False,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "\n",
        "        super(DDPG, self).__init__(\n",
        "            policy=policy,\n",
        "            env=env,\n",
        "            learning_rate=learning_rate,\n",
        "            buffer_size=buffer_size,\n",
        "            learning_starts=learning_starts,\n",
        "            batch_size=batch_size,\n",
        "            tau=tau,\n",
        "            gamma=gamma,\n",
        "            train_freq=train_freq,\n",
        "            gradient_steps=gradient_steps,\n",
        "            action_noise=action_noise,\n",
        "            replay_buffer_class=replay_buffer_class,\n",
        "            replay_buffer_kwargs=replay_buffer_kwargs,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            create_eval_env=create_eval_env,\n",
        "            seed=seed,\n",
        "            optimize_memory_usage=optimize_memory_usage,\n",
        "            # Remove all tricks from TD3 to obtain DDPG:\n",
        "            # we still need to specify target_policy_noise > 0 to avoid errors\n",
        "            policy_delay=1,\n",
        "            target_noise_clip=0.0,\n",
        "            target_policy_noise=0.1,\n",
        "            _init_setup_model=False,\n",
        "        )\n",
        "\n",
        "        # Use only one critic\n",
        "        if \"n_critics\" not in self.policy_kwargs:\n",
        "            self.policy_kwargs[\"n_critics\"] = 1\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 4,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"DDPG\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "    ) -> OffPolicyAlgorithm:\n",
        "\n",
        "        return super(DDPG, self).learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            eval_env=eval_env,\n",
        "            eval_freq=eval_freq,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            tb_log_name=tb_log_name,\n",
        "            eval_log_path=eval_log_path,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "        )"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTEdH3tPh-Ts"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcv-D0FZn8-6",
        "outputId": "c74b7368-f4d5-4079-dca5-2fb136f30321"
      },
      "source": [
        "# Agent\n",
        "from stable_baselines3 import HerReplayBuffer, SAC\n",
        "# from stable_baselines3.common.buffers import ReplayBuffer, DictReplayBuffer\n",
        "from sb3_contrib import TQC\n",
        "\n",
        "\n",
        "env = gym.make(\"parking-v0\")\n",
        "her_kwargs = dict(n_sampled_goal=4, goal_selection_strategy='future', \n",
        "                  online_sampling=True, max_episode_length=100)\n",
        "\n",
        "# You can replace TQC with SAC agent\n",
        "# model = TQC('MultiInputPolicy', env, replay_buffer_class=HerReplayBuffer,\n",
        "#             replay_buffer_kwargs=her_kwargs, verbose=1, buffer_size=int(1e6),\n",
        "#             learning_rate=1e-3,\n",
        "#             gamma=0.95, batch_size=1024, tau=0.05,\n",
        "#             policy_kwargs=dict(net_arch=[512, 512, 512]))\n",
        "\n",
        "model = DDPG(policy='MultiInputPolicy',\n",
        "             env=env,\n",
        "             verbose=1,\n",
        "            #  replay_buffer_class=DictReplayBuffer,\n",
        "             replay_buffer_class=HerReplayBuffer,\n",
        "             replay_buffer_kwargs=her_kwargs\n",
        "             )\n",
        "\n",
        "model.learn(int(5e4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 100      |\n",
            "|    ep_rew_mean     | -60      |\n",
            "|    success rate    | 0        |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 53       |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total timesteps | 400      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.953    |\n",
            "|    critic_loss     | 0.0306   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 200      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 100      |\n",
            "|    ep_rew_mean     | -58.2    |\n",
            "|    success rate    | 0        |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.06     |\n",
            "|    critic_loss     | 0.0139   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 600      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 100      |\n",
            "|    ep_rew_mean     | -54.9    |\n",
            "|    success rate    | 0        |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 44       |\n",
            "|    time_elapsed    | 26       |\n",
            "|    total timesteps | 1200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.54     |\n",
            "|    critic_loss     | 0.0222   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1000     |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpNthoZNpD-h"
      },
      "source": [
        "# Test the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgIbtcHv2MoY"
      },
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rewSb5t3pCOD"
      },
      "source": [
        "env = gym.make(\"parking-v0\")\n",
        "env = Monitor(env, './video', force=True, video_callable=lambda episode: True)\n",
        "for episode in trange(3, desc=\"Test episodes\"):\n",
        "    obs, done = env.reset(), False\n",
        "    env.unwrapped.automatic_rendering_callback = env.video_recorder.capture_frame\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video('./video')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9tawszt2NbP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}